{
  "Python - FastAPI (INTERMEDIATE), PostgreSQL (INTERMEDIATE)": [
"You have inherited a FastAPI e-commerce service with 500K+ daily transactions. The service handles user orders, inventory management, and payment processing through multiple endpoints. The PostgreSQL database contains tables for users (2M records), products (100K records), orders (5M records), and order_items (15M records). Recent performance monitoring shows API response times degrading from 200ms to 3-5 seconds during peak hours. Your task is to analyze the existing database schema, identify bottlenecks using PostgreSQL diagnostic tools, implement appropriate indexing strategies, optimize slow queries, and configure connection pooling. You must maintain data consistency while improving concurrent transaction handling and reducing lock contention. Deliverables include performance analysis report, optimized schema with migration scripts, and load testing results showing improved response times.",
"A FastAPI microservice handles real-time analytics for a social media platform with 1M+ active users. The service processes user interactions, content recommendations, and engagement metrics through async endpoints. The PostgreSQL database stores user_activities (50M records), content_metrics (20M records), and recommendation_cache (10M records) tables. The current setup experiences frequent deadlocks, high CPU usage during analytical queries, and memory issues during bulk data processing. Your objective is to redesign the database architecture using partitioning strategies, implement proper MVCC handling, optimize complex analytical queries with window functions and CTEs, configure autovacuum settings, and establish monitoring dashboards. You need to ensure the async FastAPI endpoints maintain sub-second response times while handling concurrent analytical workloads.",
"Your team runs a FastAPI-based logistics management system processing shipment tracking, route optimization, and delivery scheduling. The PostgreSQL database manages shipments (3M records), routes (500K records), vehicles (50K records), and tracking_events (25M records) tables. The system faces challenges with geospatial queries taking 10+ seconds, frequent timeout errors during batch processing, and inconsistent performance across different geographical regions. You must optimize PostGIS-enabled spatial queries, implement efficient indexing for time-series data, design proper partitioning for historical data, tune memory parameters for complex joins, and establish replication for read-heavy operations. The solution should maintain ACID compliance while supporting real-time tracking updates and historical reporting requirements.",
"A FastAPI financial services application manages trading operations, portfolio analysis, and risk calculations with strict regulatory compliance requirements. The PostgreSQL database contains trades (10M records), portfolios (1M records), market_data (100M records), and audit_logs (50M records) tables. Current issues include slow end-of-day batch processing (6+ hours), inconsistent backup completion times, and audit trail queries timing out. Your task involves optimizing bulk data processing workflows, implementing proper transaction isolation levels, designing efficient archival strategies, configuring point-in-time recovery, and establishing comprehensive monitoring for compliance reporting. You must ensure zero data loss, maintain audit trail integrity, and optimize both OLTP and OLAP workloads within the same system.",
"You're optimizing a FastAPI-powered content management system serving a news platform with high read/write ratios. The service handles article publishing, user comments, search functionality, and content moderation through various endpoints. The PostgreSQL database stores articles (5M records), comments (50M records), tags (100K records), and search_indexes (75M records) tables. Performance issues include slow full-text search queries, high replication lag during content publishing spikes, and memory exhaustion during content migration tasks. Your mission is to implement efficient full-text search with proper indexing, optimize read replicas for content delivery, design caching strategies that work with FastAPI's async nature, configure logical replication for content syndication, and establish proper vacuum strategies for high-churn tables. The solution must support real-time content publishing while maintaining search relevance and system responsiveness."
  ],
  "NextJs (INTERMEDIATE), TypeScript (INTERMEDIATE)": [
    "Develop a blog platform that showcases intermediate-level skills in both technologies: Implement both SSG for static blog posts and SSR for dynamic content (comments, likes), use conditional types, mapped types, and utility types for blog post data transformation, set up i18n with TypeScript for multi-language support, implement dynamic imports for blog post editor component, create custom React hooks with proper TypeScript typing for data fetching and caching, use CSS Modules or styled-components with TypeScript theme support, and integrate basic performance monitoring with proper error boundaries. Time limit: 30 minutes."
   ],
 "Java (INTERMEDIATE), Kafka (INTERMEDIATE)": [  
    "Production user analytics pipeline shows growing consumer lag and incorrect offset management. The Kafka consumer configuration needs optimization—switching from auto-commit to manual synchronous commits, implementing proper commit strategies (commitSync vs commitAsync), and configuring session.timeout.ms and max.poll.interval.ms correctly. The Java consumer code needs minor refactoring to replace inefficient String concatenation with StringBuilder and add proper offset commit calls after processing batches."
  ],
    "React Frameworks (INTERMEDIATE)": [
    "Build a reusable Dropdown component with keyboard navigation (arrow keys, enter, escape) and custom hook for positioning logic. Implement click outside detection, proper ARIA attributes, and support for both single and multi-select modes with controlled/uncontrolled patterns.",
    "Design a dynamic Table component that accepts column definitions as props and renders sortable headers. Implement sorting logic with custom hooks, row selection functionality, and demonstrate render props pattern for custom cell rendering with at least one complex cell type.",
    "Create an Accordion component using compound component pattern (Accordion.Item, Accordion.Header, Accordion.Panel) with custom hooks for state management. Include smooth CSS transitions, keyboard navigation, and support for both single and multiple panel expansion modes.",
    "Create a Toast notification component with custom hook for managing notification queue, auto-dismiss functionality, and different notification types (success, error, warning). Implement positioning logic and smooth enter/exit animations using CSS-in-JS.",
    "Build a Slider component with custom hooks for drag handling, range selection support, and accessibility features (ARIA attributes, keyboard controls). Implement value formatting, step increments, and proper touch gesture support for mobile devices.",
    "Implement a TreeNode component that supports nested rendering, expand/collapse functionality, and custom node content. Use recursion for nested structure rendering, custom hooks for tree state management, and demonstrate with a simple file explorer example."
   ],
  "PostgreSQL (INTERMEDIATE)": [
  "Debug and resolve a production issue where long-running analytical queries are blocking critical OLTP operations. Use pg_locks and pg_stat_activity to identify blocking relationships and blocking_pids, implement statement_timeout and lock_timeout appropriately, and tune shared_buffers, effective_cache_size, and work_mem for mixed workload. Create materialized views with appropriate refresh strategies for heavy analytical queries, implement parallel query optimization by tuning max_parallel_workers_per_gather, and use EXPLAIN ANALYZE to validate that queries utilize index-only scans where possible. Set up pg_stat_statements and auto_explain to continuously monitor query performance."  
  ],
  "SQL(INTERMEDIATE)": [
    "You are a data analyst at a BFSI company with a transactional database containing: 'transactions' (transaction_id BIGINT, account_id INT, transaction_type VARCHAR, amount DECIMAL, transaction_timestamp TIMESTAMP, status VARCHAR, merchant_category VARCHAR) and 'accounts' (account_id INT PRIMARY KEY, customer_id INT, account_type VARCHAR, current_balance DECIMAL, risk_rating VARCHAR). Analyze the last 90 days of transaction data to identify suspicious patterns:  Use window functions (ROW_NUMBER, LAG, LEAD) to detect accounts with unusual transaction frequency or amounts compared to their historical patterns,  Create a stored procedure that calculates rolling 7-day transaction volumes per account and flags accounts exceeding 3 standard deviations,  Implement proper transaction management with isolation levels for bulk updates to risk ratings,  Write an optimized query using CTEs and EXPLAIN plan to identify merchant categories with highest fraud rates. Propose indexing strategy and schema modifications to improve query performance. The solution must handle concurrent access scenarios and maintain data consistency.",
    "Your e-commerce company needs a comprehensive ETL pipeline for inventory management. You have tables: 'products' (product_id INT, sku VARCHAR, product_name VARCHAR, category_id INT, base_price DECIMAL, last_updated TIMESTAMP), 'inventory' (inventory_id INT, product_id INT, warehouse_id INT, quantity INT, reorder_level INT), 'sales_orders' (order_id INT, product_id INT, quantity_sold INT, order_date DATE, order_status VARCHAR), and 'warehouses' (warehouse_id INT, warehouse_name VARCHAR, location VARCHAR, capacity INT). Design and implement:  A MERGE statement to synchronize inventory levels from multiple source systems,  Complex queries using multiple joins, subqueries, and aggregate functions to calculate inventory turnover ratios and identify slow-moving products by category and warehouse,  Window functions to rank products by sales velocity within each category,  A stored procedure that automates reorder processes with proper error handling and transaction management. Integrate this with a Python script for data validation. Optimize the queries using indexes and partitioning strategies, and provide EXPLAIN plans showing performance improvements.",
    "You work for a healthcare analytics company with normalized tables following 3NF: 'patients' (patient_id INT, patient_name VARCHAR, dob DATE, insurance_provider VARCHAR), 'appointments' (appointment_id INT, patient_id INT, doctor_id INT, appointment_datetime TIMESTAMP, status VARCHAR, consultation_type VARCHAR), 'doctors' (doctor_id INT, doctor_name VARCHAR, specialty VARCHAR, department_id INT), 'prescriptions' (prescription_id INT, appointment_id INT, medication_name VARCHAR, dosage VARCHAR, prescribed_date DATE), and 'departments' (department_id INT, department_name VARCHAR, location VARCHAR). Build analytical queries to:  Use window functions (RANK, DENSE_RANK, NTILE) to identify top-performing doctors by patient volume and satisfaction within each specialty,  Create complex CTEs to analyze patient visit patterns and appointment no-show rates across different time windows,  Develop a stored function that calculates patient lifetime value considering appointment frequency and prescription history,  Implement bulk data operations to update appointment statuses with proper locking mechanisms and isolation levels. Profile queries using execution plans, identify bottlenecks, and recommend schema optimizations including materialized views and appropriate indexes for read-heavy workloads.",
    "As a database engineer for a financial services platform, you manage: 'loan_applications' (application_id INT, customer_id INT, loan_amount DECIMAL, interest_rate DECIMAL, application_date DATE, approval_status VARCHAR, credit_score INT), 'loan_disbursements' (disbursement_id INT, application_id INT, disbursed_amount DECIMAL, disbursement_date DATE, repayment_schedule_id INT), 'repayments' (repayment_id INT, disbursement_id INT, repayment_amount DECIMAL, repayment_date DATE, payment_status VARCHAR, days_overdue INT), and 'customers' (customer_id INT, customer_name VARCHAR, segment VARCHAR, risk_category VARCHAR). Develop:  Complex analytical queries using window functions (SUM OVER, AVG OVER with PARTITION BY) to calculate customer payment behavior metrics and delinquency trends,  A stored procedure with dynamic SQL that generates loan performance reports filtered by multiple parameters with proper SQL injection prevention,  ETL logic using MERGE and bulk INSERT operations to load daily repayment data from external sources with data validation and error logging,  Queries that identify customers eligible for loan refinancing based on improved credit scores and payment history using correlated subqueries and EXISTS clauses. Implement role-based access controls (GRANT/REVOKE), optimize for cloud database deployment (AWS RDS/Azure SQL), and provide comprehensive performance tuning recommendations with execution plans and index usage analysis.",
    "You're building a data warehouse for a retail analytics platform with fact and dimension tables: 'fact_sales' (sale_id BIGINT, product_id INT, store_id INT, customer_id INT, sale_date DATE, quantity INT, unit_price DECIMAL, discount_amount DECIMAL, tax_amount DECIMAL, total_amount DECIMAL), 'dim_products' (product_id INT, product_name VARCHAR, category VARCHAR, brand VARCHAR, supplier_id INT), 'dim_stores' (store_id INT, store_name VARCHAR, city VARCHAR, region VARCHAR, store_type VARCHAR), 'dim_customers' (customer_id INT, customer_name VARCHAR, customer_segment VARCHAR, loyalty_tier VARCHAR, registration_date DATE), and 'dim_time' (date_id DATE, year INT, quarter INT, month INT, week INT, day_of_week VARCHAR). Create:  Complex analytical queries using advanced window functions (CUME_DIST, PERCENT_RANK, FIRST_VALUE, LAST_VALUE) to perform cohort analysis and customer segmentation,  Stored procedures that calculate year-over-year growth, moving averages, and seasonal trends with parameters for dynamic time periods,  Materialized views for frequently accessed aggregations with incremental refresh strategies,  ETL pipeline using MERGE statements and CTEs to handle slowly changing dimensions (SCD Type  for product and customer data. Implement partitioning strategies for the large fact table, create appropriate indexes (clustered, non-clustered, columnstore), and provide comprehensive performance optimization including query hints, statistics management, and execution plan analysis. Ensure the solution handles concurrent analytical queries efficiently and integrates with Python/shell scripts for automated data pipeline orchestration."
  ],
  "Java (INTERMEDIATE)": [
    "A microservices transaction coordinator has a deadlock problem: ServiceA locks Resource-1 then tries to lock Resource-2, while ServiceB does the reverse. The current implementation uses ReentrantLock with tryLock(). Your task is to: 1) Fix the deadlock by implementing ordered lock acquisition - always acquire locks in sorted order by resource ID (e.g., alphabetically), 2) Add timeout-based failure detection - use tryLock(5, TimeUnit.SECONDS) and if any lock acquisition fails, release all held locks and abort the transaction, 3) Wrap the transaction logic in a CompletableFuture to make it non-blocking and return success/failure status, and 4) Add basic retry logic - if a transaction fails due to lock timeout, retry up to 3 times with a 100ms delay between attempts. Create a test with 10 concurrent threads attempting transactions on 5 shared resources to verify no deadlocks occur and all transactions either complete or fail gracefully."
  ],
  "React Native(INTERMEDIATE)": [
    "You're building a 'Product Catalog' feature for an existing e-commerce app (TypeScript setup already provided). The app currently has Redux Toolkit configured with basic slices. Your task:  Create a performant product listing screen that handles 1000+ items - implement FlatList with proper optimization techniques (getItemLayout, React.memo for item component, useCallback for item press handlers) and measure the performance impact using React DevTools Profiler;  Implement offline-first functionality using WatermelonDB - set up a products table, sync fetched API data to local database, and display products from local DB when offline with a sync indicator;  Add a custom native module (Swift for iOS OR Kotlin for Android, choose one platform) that analyzes product images using the device's ML capabilities and returns dominant colors - bridge this to React Native and display the color palette on product cards. The existing navigation, API endpoints, and basic Redux structure are already implemented.",
    
    "You're enhancing a 'Delivery Tracking' app that already has basic navigation and authentication set up. Your focus areas:  Implement real-time location tracking - integrate react-native-maps with animated marker that smoothly moves as delivery location updates (use Reanimated 2 for 60fps animation interpolation between coordinates), mock location updates with a timer for demonstration;  Add background location capability using Headless JS - implement a background task that continues tracking even when app is in background, properly handle AppState changes and display background status indicator;  Create a custom Flipper plugin to debug and visualize the location update queue - plugin should show pending location updates, sync status, and allow manual trigger of sync operations. The app already has proper permission handling and basic map integration, focus on the advanced implementations listed.",
    
    "Working on a 'Healthcare Portal' app with existing TypeScript, React Navigation, and basic authentication. Your implementation tasks:  Build a secure document upload feature - implement image compression before upload, use react-native-keychain to securely store upload tokens, and add certificate pinning for the upload API endpoint (configuration-level implementation with proper error handling);  Implement proper Error Boundaries with fallback UI for three critical sections (patient data, documents, appointments) - each boundary should log errors to a mock Sentry service and provide recovery actions;  Create a complex nested form for patient intake using react-hook-form with TypeScript - form should have validation, conditional fields based on previous answers, proper accessibility labels, and use Suspense to lazy-load the form sections. The API endpoints, navigation structure, and design system are already in place.",
    
    "You're optimizing an existing 'Social Media Feed' app that's experiencing performance issues. The app has basic Redux, navigation, and API integration. Your tasks:  Implement performance profiling and optimization - use Flipper and React DevTools Profiler to identify re-render issues in the feed, apply proper memoization techniques (React.memo, useCallback, useMemo) to optimize the FlatList rendering, and document performance metrics before/after;  Add CodePush OTA update capability - integrate CodePush SDK, configure staging and production deployments, implement version checking with user-facing update prompt (mandatory vs optional updates);  Set up proper monitoring and debugging infrastructure - integrate Sentry with custom breadcrumbs for user actions, add Flipper network inspector for API debugging, and create custom Hermes profiling configuration in Metro bundler. Focus on making measurable performance improvements and setting up production-ready monitoring.",
    
    "Enhancing a 'Smart Home Control' app with existing device list and basic controls. Your implementation focus:  Create smooth, gesture-based device controls - use react-native-gesture-handler and Reanimated 2 to build interactive slider controls for dimmer lights (pan gesture with spring animation), rotary knob for thermostat (rotation gesture with haptic feedback), and swipe gestures for scene activation - all animations must maintain 60fps;  Implement an offline-first architecture for device commands - use MMKV for fast state storage, create a command queue system that stores failed commands and retries when connection is restored, use NetInfo to monitor connectivity and show appropriate UI states;  Add proper TypeScript architecture with dependency injection - refactor device communication logic into injectable services following SOLID principles, create custom hooks that use these services (useDeviceControl, useDeviceState), and implement proper error handling with typed errors. The MQTT/device communication layer and basic UI are already implemented."
  ],
  "NodeJS(INTERMEDIATE)": [
    "You're working on an existing Express.js user management API that already has basic routes and MongoDB setup. Your task:  Implement JWT-based authentication middleware that protects specific routes and validates tokens,  Add password hashing using bcrypt for the registration endpoint (already structured), and  Implement input sanitization middleware to prevent NoSQL injection attacks on user input fields. The database connection and basic CRUD routes are already provided - focus on adding these three security layers.",
    "A microservice for processing user uploads is experiencing issues. You have an existing Express API with upload endpoints. Your tasks:  Implement Redis caching for frequently accessed file metadata (GET /files/:id) with a 5-minute TTL,  Add structured logging using Winston to log all file operations with request IDs, and  Implement a simple health check endpoint (/health) that verifies Redis and file storage connectivity. The base application structure and upload logic are already in place.",
    "You've inherited an Express API that connects to PostgreSQL. Your goal is to optimize and enhance it:  Refactor one existing endpoint to use connection pooling properly and add a database transaction for data consistency,  Implement a custom error handling middleware that catches database errors and returns standardized JSON error responses with appropriate status codes, and  Add request correlation IDs to track requests across the application. Focus on these three production-ready enhancements.",
    "An existing REST API needs performance and reliability improvements. You have the base Express application with several endpoints. Implement:  A circuit breaker pattern for one external API call (mock service can be used) that opens after 3 failures and closes after 30 seconds,  Response caching middleware using in-memory cache or Redis for GET endpoints with configurable TTL, and  Rate limiting per API key (stored in headers) that allows 100 requests per 15 minutes. The application structure and routes already exist.",
    "You're enhancing an existing multi-tenant SaaS API. The basic Express app with PostgreSQL is set up. Your tasks:  Implement tenant identification middleware that extracts tenant ID from request headers and attaches it to the request object,  Add role-based access control (RBAC) middleware that checks user permissions against required roles for specific routes, and  Implement audit logging that records all POST/PUT/DELETE operations with user info, tenant ID, and timestamp to a separate audit table. Focus on these three middleware implementations - the database models and basic routes are provided."
  ],
  "Python-FastAPI(INTERMEDIATE),Kafka(INTERMEDIATE)": [
    "You are working on a real-time order processing system where orders are published to a Kafka topic 'orders-incoming' and processed by a FastAPI microservice. The system is experiencing high consumer lag (>50,000 messages) during peak hours and occasional duplicate order processing. Your task is to:  Analyze the existing consumer configuration and identify bottlenecks in partition consumption strategy,  Implement idempotent processing using Kafka transactional capabilities or application-level deduplication with Redis,  Optimize the FastAPI consumer service by implementing proper async/await patterns and connection pooling for database writes,  Configure appropriate batch processing with manual offset commits to balance throughput and safety, and  Add monitoring metrics (consumer lag, processing rate, errors) using Prometheus. The solution should handle at least 1000 messages/second with lag under 5000 messages. Provide the optimized consumer code, Kafka configuration, and a brief explanation of the delivery guarantees achieved.",
    "Your team maintains a FastAPI-based event streaming platform that consumes from multiple Kafka topics with different schemas (Avro format in Schema Registry). The current implementation is causing frequent ISR shrinkage and uneven partition distribution across brokers. Your task is to:  Investigate and fix the producer configuration causing ISR issues (examine acks, retries, max.in.flight.requests.per.connection settings),  Implement a FastAPI endpoint that publishes events with proper error handling and retry logic using transactions for exactly-once semantics,  Design a consumer group with cooperative rebalancing strategy that processes events from 3 topics ('user-events', 'payment-events', 'notification-events') with schema evolution support,  Add Avro schema validation and compatibility checks in the FastAPI service using Schema Registry client, and  Implement proper offset management with periodic commits and lag monitoring. Include producer/consumer configurations, FastAPI route handlers with Pydantic models mapped to Avro schemas, and explain your partition key selection strategy.",
    "A financial transaction processing system using FastAPI and Kafka is experiencing performance degradation and data inconsistencies. Transactions are produced to 'transactions' topic (log compacted) and consumed by multiple microservices. Current issues include: slow producer throughput (~200 msgs/sec), high GC pauses on broker JVMs, and occasional data loss during broker failures. Your task is to:  Optimize Kafka producer settings (batching, linger.ms, compression, buffer.memory) integrated in FastAPI endpoints,  Tune broker configurations (log.segment.bytes, num.replica.fetchers, socket.request.max.bytes) to reduce GC pressure,  Implement a FastAPI consumer with proper error handling, circuit breaker pattern using tenacity library, and dead-letter topic routing for failed messages,  Design a background job system using FastAPI BackgroundTasks for async transaction validation that doesn't block the HTTP response, and  Add OpenTelemetry tracing to track message flow from producer through Kafka to consumer. Provide optimized configurations, FastAPI service code with dependency injection for Kafka clients, and explain the trade-offs in your compression and batching choices.",
    "You're building a real-time analytics pipeline where FastAPI services produce events to Kafka topics partitioned by user_id, and downstream consumers aggregate data using Kafka Streams. The system faces challenges: uneven partition load (some partitions 10x larger), consumer rebalancing storms during deployments, and missing security controls. Your task is to:  Implement a custom partitioning strategy in the FastAPI producer to achieve better load distribution while maintaining ordering guarantees per user,  Configure consumer groups with static membership and cooperative-sticky assignment to minimize rebalancing impact,  Set up end-to-end security with SASL/SCRAM authentication and ACLs for topic access, integrating credentials management via environment variables in FastAPI,  Implement request-level rate limiting in FastAPI using middleware and Kafka quotas to prevent producer/consumer abuse,  Create a FastAPI admin endpoint that uses AdminClient to monitor topic health (partition count, ISR status, under-replicated partitions) and expose metrics. Include producer with custom partitioner, secured consumer configuration, FastAPI security middleware, and explain your approach to maintaining message ordering with the new partitioning strategy.",
    "An e-commerce platform uses FastAPI microservices with Kafka for event-driven communication. The current architecture has multiple reliability and observability gaps: messages occasionally processed out of order, no visibility into processing failures, and inefficient database operations causing timeout errors. Your task is to:  Design and implement an outbox pattern using SQLAlchemy transactions in FastAPI to ensure atomic database writes and Kafka publishes,  Configure Kafka consumers with proper session.timeout.ms, heartbeat.interval.ms, and max.poll.interval.ms to prevent unnecessary rebalances during slow processing,  Implement structured logging with correlation IDs that track requests from FastAPI endpoint through Kafka messages to consumers,  Create a retry mechanism with exponential backoff for transient failures and a dead-letter queue for permanent failures, with a FastAPI admin interface to inspect and replay failed messages, and  Optimize database operations using SQLAlchemy with connection pooling, async database drivers, and batch inserts where appropriate. Provide complete FastAPI service code with database models, Kafka producer/consumer implementations with proper offset management, and a monitoring strategy using custom exception handlers to track error rates by error type."
  ],
   "Java(INTERMEDIATE),Redis(INTERMEDIATE)": [
    "You are working on an e-commerce platform experiencing performance bottlenecks during flash sales. The current implementation uses Redis String data type to cache individual product details with a 5-minute TTL, causing excessive cache misses and database load. Your task:  Analyze the provided Java service code that fetches product inventory and pricing data,  Redesign the caching strategy using Redis Hash and Pipeline operations to batch-retrieve multiple products in a single round-trip,  Implement proper key naming conventions with category prefixes and configure appropriate TTL/eviction policies for different product types (trending vs. regular),  Add basic monitoring to track cache hit ratios using Redis INFO stats, and  Ensure thread-safe cache updates using Redis WATCH for optimistic locking when inventory decrements occur. Provide the refactored Java code with Redis integration and explain your design decisions regarding data structure selection and concurrency handling.",
    "A microservices-based order processing system is experiencing race conditions where multiple services simultaneously update order status in Redis, leading to inconsistent states. Currently, each service uses simple SET/GET operations with a Sorted Set for order prioritization by timestamp. Your task:  Review the provided Java Spring Boot service code and identify concurrency issues,  Implement distributed locking using Redis SET NX EX with proper timeout and token-based fencing to prevent deadlocks,  Refactor the order status workflow using Redis MULTI/EXEC transactions or Lua scripting to ensure atomic state transitions (pending → processing → completed),  Optimize the priority queue implementation by combining Sorted Sets with Hashes to store both order metadata and scores efficiently,  Configure Redis connection pooling (Jedis/Lettuce) with appropriate timeout, retry logic, and circuit breaker patterns using Resilience4j. Deliver production-ready code demonstrating ACID guarantees and explain how your solution handles failure scenarios like network partitions or Redis instance restarts.",
    "Your team manages a session store for a high-traffic web application currently using Redis Strings serialized as JSON, consuming 12GB memory with frequent eviction warnings. Sessions contain user preferences, cart items, and authentication tokens with varying access patterns. Your task:  Analyze memory usage using Redis MEMORY DOCTOR and INFO commands to identify inefficient patterns,  Redesign the session storage using Redis Hash data type with field-level TTL considerations and HyperLogLog for tracking unique daily active users,  Implement a Java-based read-through/write-through caching layer using Spring Cache abstraction with appropriate serialization (JSON vs. Java Serialization vs. Protobuf trade-offs),  Configure Redis maxmemory policy (allkeys-lru vs. volatile-lru) and active defragmentation parameters to optimize memory footprint by at least 40%,  Set up Prometheus exporter integration in your Java application to expose custom metrics (cache operations, serialization time, memory per session type) and create alerting thresholds for eviction rates exceeding 100/min. Provide the optimized Java code, Redis configuration adjustments, and a brief capacity planning analysis.",
    "A real-time analytics dashboard aggregates user activity events (clicks, views, purchases) from multiple services, but the current Redis Stream implementation causes consumer lag during peak hours (10K events/sec). The Java consumer group processes events sequentially with blocking reads. Your task:  Review the existing Java consumer code using XREADGROUP and identify bottlenecks in event processing and acknowledgment patterns,  Refactor the solution to implement parallel Stream consumer groups with proper claim/acknowledge mechanisms using XPENDING and XCLAIM for handling crashed consumers,  Implement back-pressure control by monitoring consumer lag (XINFO GROUPS) and dynamically adjusting batch sizes and blocking timeouts in Java,  Add a secondary Redis Bitmap structure to track processed event IDs for idempotency guarantees across restarts,  Optimize the Java processing pipeline using ExecutorService with bounded queues and implement graceful shutdown with proper XACK batching. Integrate SLOWLOG and LATENCY monitoring to ensure p99 latency stays under 50ms. Deliver the complete Java consumer application with configuration for horizontal scaling and explain your strategy for handling duplicate events and ensuring exactly-once semantics.",
    "Your company is migrating a legacy Java application's caching layer from Memcached to Redis Cluster. The application performs heavy read operations on user profiles (500K keys, avg 2KB each) with 80:20 read/write ratio. Your task:  Design a Redis Cluster deployment strategy determining optimal shard count, replication factor, and hash slot distribution for the given workload,  Implement a Java service using Lettuce or Jedis Cluster client with smart routing, handling MOVED/ASK redirections and slot cache refresh mechanisms,  Create a zero-downtime migration pipeline that uses Redis SCAN to iterate through existing data, batch-migrates using PIPELINE commands, and implements dual-write pattern with rollback capability,  Configure RDB + AOF hybrid persistence with tuned appendfsync settings to balance durability vs. performance, and set up automated backup to external storage (S3 simulation),  Implement comprehensive error handling for cluster topology changes (node addition/removal), connection pool exhaustion, and failover scenarios using retry policies and circuit breakers. Provide the Java migration utility, client configuration code, and a runbook documenting rollback procedures and performance benchmarking results comparing before/after latency percentiles."
  ],
   "Python - FastAPI (INTERMEDIATE), Redis (INTERMEDIATE)": [
      "Build a distributed circuit breaker for an external payment API. Using Redis hashes, track failure counts, last failure time, and circuit state (closed/open/half-open) per payment provider. Implement a decorator that: checks circuit state before calling payment API, increments failure count on errors, opens circuit after 5 failures in 1 minute, auto-transitions to half-open after 30 seconds, and closes circuit after successful half-open test. Use Lua script for atomic state transitions. Include endpoint to manually reset circuit."
  ],
  "MongoDB (INTERMEDIATE), NodeJs (INTERMEDIATE), React Framework (INTERMEDIATE)": [
    "Develop a commenting system with nested replies and real-time updates for a blog platform. Implement: (1) A MongoDB schema using the extended-reference pattern where comments embed author info but reference the parent post, supporting nested replies up to 3 levels deep, (2) A Node.js API with endpoints for creating comments, fetching comment threads with $graphLookup for nested replies, and implementing a Change Stream to emit real-time comment notifications, (3) A React component tree that renders nested comments recursively, includes a form with Formik validation for new comments/replies, and uses custom hooks to subscribe to real-time updates via WebSocket or polling, (4) Implement pagination for top-level comments using range-based cursors. Apply proper indexing strategy and explain() analysis to optimize the nested query performance."
  ],
  "MongoDB (INTERMEDIATE), NodeJs (INTERMEDIATE)": [
    "Your multi-tenant SaaS application's API built with Express and Mongoose is slowing down as tenant data grows unevenly (10K to 2M+ documents per tenant). The current architecture uses a single MongoDB collection with tenantId field, queries use skip/limit pagination causing slow page loads on large offsets, Node.js route handlers don't implement proper request timeouts, and there's no query result caching. Optimize both MongoDB (implement partial indexes on tenantId + active status, refactor pagination from skip/limit to cursor-based range seeking using indexed fields, apply bucket pattern for tenant time-series data, configure appropriate read preferences for analytics vs transactional queries) and Node.js (implement request-level caching with Redis using tenant-aware cache keys, add query timeout middleware, refactor pagination API to support cursor tokens in Express routes, implement lazy loading with streaming responses for large datasets, add monitoring middleware to track per-tenant query performance). Include updated API endpoint code, MongoDB index definitions, explain plans, and performance benchmarks showing improvement for both small and large tenants."  
  ],
  "ExpressJS (INTERMEDIATE)": [
    "Build a production-ready Task Management API with PostgreSQL/MongoDB integration. Implement layered architecture (routes → controllers → services → data access). Create REST endpoints for tasks, projects, and users with full CRUD, pagination, and filtering (query builders with Knex/Mongoose). Use async/await throughout with centralized error handling middleware. Integrate Helmet for security headers, implement rate limiting (express-rate-limit), CORS configuration, and input validation/sanitization (express-validator/joi). Write a Dockerfile with multi-stage build, configure environment variables via dotenv, and include JWT authentication middleware with role-based access (admin can delete any task, users only their own). Implement connection pooling and basic transaction handling for task creation with project updates."
    ],
  "Java (INTERMEDIATE), Docker (INTERMEDIATE)": [
    "A Spring Boot microservice's Docker container is crashing with OOMKilled errors, and the 450MB image causes slow deployments. You're provided with a working Spring Boot application JAR. Your tasks:  Create a multi-stage Dockerfile that uses eclipse-temurin:11-jdk for building and eclipse-temurin:11-jre-alpine for runtime,  Configure JVM memory settings using -XX:MaxRAMPercentage=75.0 in the ENTRYPOINT,  Set up docker-compose.yml with resource limits (--memory=512m, --cpus=1.0) and environment variables for database URL and thread pool size,  Add a HEALTHCHECK directive that curls the existing /actuator/health endpoint, Configure Docker logging driver to json-file with rotation. You'll need to modify the application's application.yml to read DATABASE_URL and THREAD_POOL_SIZE from environment variables, then demonstrate the container running stably with proper resource constraints and verify the image size reduction from 450MB to under 150MB."
  
  ],
  "Java - Multithread Programming(INTERMEDIATE),Java - Distributed Systems Concurrency(INTERMEDIATE)": [
    "You are working on a distributed order processing system where multiple services produce order events and a consumer service processes them. The current implementation uses a single-threaded consumer that's becoming a bottleneck - it can only process 50 orders/second, but the system now receives 200+ orders/second during peak hours. The consumer reads from a BlockingQueue, validates orders, calls an external payment service (simulated 100ms latency), and updates order status. Your task is to: 1) Refactor the consumer to use a thread pool (ExecutorService) for parallel processing while maintaining thread safety, 2) Ensure that order status updates don't have race conditions using appropriate synchronization or atomic operations, 3) Implement proper exception handling so one failing order doesn't crash the entire consumer, and 4) Add a mechanism using CountDownLatch or similar to gracefully shutdown the consumer and wait for in-flight orders to complete. Measure and demonstrate the throughput improvement.",
    
    "A legacy inventory management system has a critical concurrency bug in production: multiple threads are updating product stock levels simultaneously, causing occasional negative inventory counts and overselling. The current code uses basic synchronized methods on a shared InventoryService class. Your task is to: 1) Identify and fix the race condition by analyzing the existing synchronization approach (you'll be given code with synchronized methods but improper transaction boundaries), 2) Refactor to use ReentrantReadWriteLock to allow concurrent reads while ensuring exclusive writes, 3) Replace the simple counter with AtomicInteger or LongAdder where appropriate for better performance, 4) Add proper validation to prevent negative stock and throw appropriate exceptions, and 5) Write a JUnit test using multiple threads (with CountDownLatch to coordinate) that demonstrates the bug is fixed even under concurrent load of 100+ threads attempting simultaneous updates.",
    
    "You're building a distributed cache synchronization component for a microservices architecture. When data is updated in one service instance, it needs to notify all other instances to invalidate their local caches. The current implementation has a deadlock issue: ServiceA holds a lock on CacheManager while trying to acquire a lock on NotificationService, while ServiceB does the reverse. Your task is to: 1) Analyze the provided code to identify the circular lock dependency causing the deadlock, 2) Refactor using proper lock ordering or eliminate nested locking by redesigning the component structure, 3) Implement a thread-safe notification queue using ConcurrentHashMap and BlockingQueue to decouple cache updates from notifications, 4) Add a configurable thread pool to process notifications asynchronously, and 5) Demonstrate using thread dumps or logging that the deadlock is resolved. The solution should handle at least 50 concurrent cache update requests without blocking.",
    
    "A distributed rate limiter service is experiencing severe performance degradation under load. The current implementation uses a synchronized HashMap to track API request counts per user, and every request acquires a global lock. With 10,000+ requests/second across 1,000+ users, threads are spending 80% of their time waiting for locks. Your task is to: 1) Refactor from synchronized HashMap to ConcurrentHashMap with atomic operations (compute, computeIfAbsent), 2) Implement a sliding window rate limiting algorithm using AtomicInteger or AtomicLong for lock-free counter updates, 3) Add a scheduled cleanup thread using ScheduledExecutorService to remove expired entries and prevent memory leaks, 4) Ensure thread safety without synchronized blocks on the hot path, and 5) Benchmark the improvement by simulating concurrent requests from multiple threads. Your solution should achieve at least 5x throughput improvement while maintaining accurate rate limiting.",
    
    "You're implementing a distributed job scheduler that assigns tasks to worker threads. The system uses a producer-consumer pattern with multiple producers (services submitting jobs) and consumers (worker threads). Currently, the system has three problems: 1) Jobs are occasionally processed twice because multiple workers grab the same job due to improper synchronization, 2) When all workers are busy, new jobs fill up memory because there's no backpressure mechanism, and 3) Worker threads don't properly handle exceptions, causing thread pool exhaustion. Your task is to: Fix the duplicate processing issue by using a thread-safe BlockingQueue with proper dequeue operations and idempotency checks, Implement bounded queue with appropriate rejection policy to prevent memory overflow, Add proper exception handling in worker threads with retry logic using ExecutorService's submit() and Future, and Implement graceful shutdown using shutdown() and awaitTermination() to ensure in-progress jobs complete. Add monitoring using thread pool metrics (active count, queue size) and demonstrate that all three issues are resolved under load."
  ],
  "Java - Distributed Systems Concurrency(intermediate)": [
    "You are working on a distributed API rate limiter that's currently too slow. Each rate limit check takes 150ms because it acquires a ReentrantLock, reads a counter from a shared ConcurrentHashMap, increments it, and releases the lock. With 100 requests/second, threads spend 90% of their time waiting for locks. Your task is to: 1) Replace the ReentrantLock + manual counter increment with AtomicInteger.incrementAndGet() for lock-free updates, 2) Add a ScheduledExecutorService that resets counters every minute (simulating a sliding window), and 3) Implement a simple fallback using Semaphore - if the counter service is 'unavailable' (simulate with a boolean flag), use a local semaphore with 100 permits for rate limiting. Write a test with 50 concurrent threads making requests to demonstrate the latency drops below 10ms per check while still enforcing the rate limit correctly.",
    
    "A Kafka-based order processing service occasionally processes the same order twice when it restarts, causing duplicate charges. The consumer reads messages, processes orders, and commits offsets - but if it crashes between processing and committing, it re-processes orders on restart. Your task is to: 1) Add idempotency by maintaining a ConcurrentHashMap of processed order IDs (use the most recent 1000 IDs as a simple cache), 2) Before processing each order, check if the order ID already exists in the map - if yes, skip processing and just commit the offset, 3) Add proper exception handling in the message processing loop so one bad message doesn't crash the entire consumer, and 4) Implement graceful shutdown using CountDownLatch - when shutdown is signaled, stop polling for new messages but finish processing current batch. Demonstrate with a test that simulates 100 messages including 10 duplicates, verifying each order is processed exactly once.",
    
    "A distributed cache has a race condition during invalidation: when Service A updates a value and broadcasts an invalidation message, Service B might receive the invalidation BEFORE receiving the new value, leaving it with no cached data when it should have the updated value. The current code uses simple ConcurrentHashMap.put() and remove(). Your task is to: 1) Add version numbers using AtomicLong - each update increments the version and includes it in both the cache value and invalidation message, 2) Modify the cache to store both value and version in a simple wrapper class, 3) Update the invalidation handler to only remove cache entries if the invalidation version is greater than or equal to the cached version (preventing premature invalidation), and 4) Add a ReadWriteLock to ensure that read operations don't see partial updates. Write a test that simulates 3 threads doing concurrent updates and invalidations to verify no stale or missing data.",
    
    "A microservices transaction coordinator has a deadlock problem: ServiceA locks Resource-1 then tries to lock Resource-2, while ServiceB does the reverse. The current implementation uses ReentrantLock with tryLock(). Your task is to: 1) Fix the deadlock by implementing ordered lock acquisition - always acquire locks in sorted order by resource ID (e.g., alphabetically), 2) Add timeout-based failure detection - use tryLock(5, TimeUnit.SECONDS) and if any lock acquisition fails, release all held locks and abort the transaction, 3) Wrap the transaction logic in a CompletableFuture to make it non-blocking and return success/failure status, and 4) Add basic retry logic - if a transaction fails due to lock timeout, retry up to 3 times with a 100ms delay between attempts. Create a test with 10 concurrent threads attempting transactions on 5 shared resources to verify no deadlocks occur and all transactions either complete or fail gracefully.",
    
    "A real-time event processing system is running out of memory because incoming events are queued in an unbounded LinkedList that grows to millions of entries when downstream processing is slow. CPU usage is also high due to inefficient single-threaded processing. Your task is to: 1) Replace LinkedList with a bounded ArrayBlockingQueue (capacity: 10,000) that blocks producers when full, preventing memory overflow, 2) Change from single-threaded processing to an ExecutorService with a FixedThreadPool of 4 threads that consume from the queue, 3) Implement batching - instead of processing events one-by-one, accumulate them in a ConcurrentHashMap and flush batches of 100 events every 2 seconds using ScheduledExecutorService to reduce processing overhead, and 4) Add graceful shutdown - call shutdown() and awaitTermination(10, TimeUnit.SECONDS) to ensure in-flight events are processed before exit. Test with a producer generating 1000 events/second to verify memory stays bounded and processing throughput improves by at least 3x."
  ],
   "Java(intermediate),asynchronous(intermediate)": [
    "Your e-commerce application needs to fetch product details, check inventory availability, and retrieve user recommendations from three different microservices. Currently, these calls are made sequentially, causing the API response time to exceed 3 seconds. Refactor the service layer to use CompletableFuture to make these calls asynchronously and aggregate the results. Handle scenarios where one or more services might fail or timeout (using orTimeout or completeOnTimeout), and ensure the response is returned with partial data if available. Implement proper exception handling and demonstrate how to combine multiple async operations efficiently.",
    
    "A notification service is currently blocking the main request thread while sending emails, SMS, and push notifications, causing poor user experience. Redesign this service using asynchronous processing with CompletableFuture or Spring's @Async annotation. Implement a callback mechanism to log notification delivery status asynchronously. Ensure that the main API responds immediately to the user while notifications are processed in the background. Add proper thread pool configuration for async tasks and demonstrate how to handle exceptions in async methods without affecting the main flow."
  ],
  "Golang (INTERMEDIATE)":[
"Your microservice has a shared connection pool that's causing race conditions and deadlocks. Implement a thread-safe connection pool with: 1) Maximum 10 connections using a semaphore pattern (buffered channel), 2) Acquire() and Release() methods protected by sync.Mutex for connection lifecycle management, 3) A background goroutine that health-checks idle connections every 5 seconds using time.Ticker, 4) Support for graceful drain during shutdown - reject new acquisitions, wait for active connections using sync.WaitGroup, then close all connections, 5) Implement proper cleanup using sync.Once for one-time initialization and defer for resource release. Provide complete code with 5 concurrent workers competing for connections "
  ],
  "GO(intermediate) + Go - Asynchronous Programming(intermediate)": [
    "You are building a web scraper that needs to fetch content from 50 URLs concurrently. Implement an asynchronous solution using goroutines and channels that: 1) Limits concurrent HTTP requests to 5 at a time using a semaphore pattern with buffered channels, 2) Uses context.WithTimeout to enforce a 3-second timeout for each request, 3) Implements the fan-in pattern to collect all successful responses and errors into separate slices, 4) Handles graceful shutdown when the parent context is cancelled. Provide complete working code with proper channel closing semantics, demonstrate how you would prevent goroutine leaks, and include comments explaining your design decisions.",
    
    "Your team's notification service needs to process incoming events asynchronously and distribute them to multiple downstream handlers. Design a pipeline with three stages: 1) Stage 1 receives raw events from an input channel, 2) Stage 2 validates and enriches events (simulate 50-100ms processing), 3) Stage 3 fans out to 3 consumer goroutines that handle events independently. Use select statements for timeout handling (2 seconds per stage), implement proper error propagation using error channels, and ensure the pipeline can be cancelled via context. The solution should handle backpressure gracefully using buffered channels and demonstrate non-blocking sends where appropriate. Include test code showing how you would verify the pipeline completes all in-flight work during shutdown.",
    
    "You need to implement an asynchronous task scheduler that executes periodic jobs concurrently. Build a system where: 1) Multiple jobs can be scheduled with different intervals (e.g., every 1s, 5s, 10s), 2) Each job runs in its own goroutine using time.Ticker, 3) Jobs can be dynamically added or removed without restarting the scheduler, 4) The scheduler supports graceful shutdown using context and sync.WaitGroup, ensuring all running jobs complete. Use select statements to handle multiple channels (ticker, context.Done(), job control). Provide working code with at least 3 sample jobs, demonstrate proper cleanup of tickers and goroutines, and explain how you would test for goroutine leaks using runtime.NumGoroutine() or pprof.",
    
    "A real-time analytics service aggregates metrics from multiple data sources asynchronously. Implement a fan-out/fan-in pattern where: 1) One producer goroutine generates metric events at varying rates (50-200ms intervals), 2) Events are distributed to 4 worker goroutines that process them independently (simulate 100-300ms processing time), 3) Processed results are collected using fan-in into a single output channel, 4) If a worker's input channel buffer is full, the producer should skip that worker (non-blocking send using select with default). Implement proper context-based cancellation, use directional channels where appropriate, and ensure no goroutines leak after shutdown. Include comments on buffering strategy and demonstrate testing with the race detector.",
    
    "You are building an asynchronous request-response system for a microservice. Implement a solution using channels where: 1) Multiple client goroutines send requests to a central processor via a request channel, 2) The processor handles requests asynchronously and sends responses back through individual response channels (using a map[requestID]chan response), 3) Clients wait for responses with a 5-second timeout using select and context.WithTimeout, 4) The system supports dynamic registration/deregistration of clients without blocking. Handle edge cases like duplicate request IDs, cleanup of abandoned response channels, and graceful shutdown using context and sync.Once. Provide complete working code with at least 3 concurrent clients, demonstrate proper synchronization for the response channel map, and explain how you would profile this using pprof to detect channel contention."
  ],
  "GO(intermediate) + Go - Multithread Programming(intermediate)": [
    "Your API service is experiencing race conditions when multiple goroutines access a shared in-memory cache (map[string]interface{}). The current implementation uses sync.Mutex for all operations, causing performance bottlenecks with 80% read and 20% write operations. Refactor the code to: 1) Use sync.RWMutex to optimize read-heavy workloads, 2) Implement thread-safe Get, Set, and Delete methods, 3) Add a background goroutine that periodically cleans up expired entries (TTL-based), 4) Use sync.WaitGroup for coordinated shutdown. Provide complete working code that simulates 10 concurrent readers and 2 writers, run it with go test -race, and explain your locking strategy. Include test cases demonstrating concurrent access patterns.",
    
    "A data processing service is suffering from deadlocks when multiple worker goroutines compete for two shared resources (database connection pool and file handle pool). The current naive implementation acquires locks in inconsistent order. Debug and fix the deadlock by: 1) Implementing a consistent lock ordering strategy, 2) Using sync.Mutex or sync.RWMutex appropriately for each resource, 3) Adding a worker pool pattern with 5 goroutines that process jobs requiring both resources, 4) Implementing proper cleanup using defer and sync.WaitGroup. Provide working code that demonstrates the deadlock scenario (with comments), the fixed version, and explain how you identified the issue. Include goroutine lifecycle management and demonstrate testing with race detector.",
    
    "You need to build a thread-safe counter service that tracks metrics across multiple goroutines. Implement a solution using: 1) sync/atomic operations for a high-frequency counter (incremented by 20 concurrent goroutines 1000 times each), 2) sync.Mutex for a map-based counter tracking multiple metric types, 3) sync.RWMutex for a cached aggregation result that's read frequently but updated rarely, 4) sync.Once to ensure initialization code runs exactly once despite concurrent access. Provide complete working code with benchmark tests (go test -bench) comparing atomic vs mutex performance, demonstrate correct synchronization with the race detector, and explain when to use each primitive. Include pprof-ready code showing goroutine contention metrics.",
    
    "Your microservice implements a producer-consumer pattern where multiple producers write to a shared queue and multiple consumers process items. The current implementation has goroutine leaks and occasional panics due to improper synchronization. Redesign using: 1) A thread-safe queue using sync.Mutex and sync.Cond for efficient blocking/signaling, 2) 3 producer goroutines and 4 consumer goroutines, 3) Graceful shutdown mechanism using a done channel and sync.WaitGroup, 4) Proper handling of queue closure to prevent panic when consumers try to read after producers finish. Provide working code with detailed comments on Cond.Wait() and Cond.Signal() usage, demonstrate no goroutine leaks using runtime.NumGoroutine(), and explain the memory model implications of your synchronization approach.",
    
    "A multi-threaded batch processing system processes items in parallel but must maintain per-user rate limits. Implement a solution where: 1) A worker pool of 8 goroutines processes jobs concurrently, 2) Each user has a rate limiter (sync.Mutex-protected map[userID]*rateLimiter), 3) Workers must acquire user-specific locks before processing to enforce rate limits, 4) Avoid deadlocks and starvation while maximizing throughput. Use sync.RWMutex for the rate limiter map access, implement proper lock acquisition ordering, and add metrics using sync/atomic. Provide complete working code simulating 100 jobs across 10 users, demonstrate with race detector, and include profiling code (pprof) to identify contention hotspots. Explain your strategy for preventing lock contention and handling slow consumers."
  ],
  "GO(intermediate) + Go - Concurrency Programming(intermediate)": [
    "You are building a concurrent log aggregator service that collects logs from multiple sources and writes them to a file. Implement a solution where: 1) 5 producer goroutines generate log entries at different rates (50-200ms intervals), 2) A single consumer goroutine batches logs and writes them every 500ms or when batch size reaches 10 entries (whichever comes first), 3) Use buffered channels for log collection and select statement for timeout-based flushing, 4) Implement graceful shutdown using context.Context ensuring all buffered logs are flushed before exit, 5) Use sync.WaitGroup to coordinate goroutine completion. Provide complete working code with proper channel closing semantics, demonstrate testing with race detector, and explain your buffering strategy and how it prevents data loss during shutdown.",
    
    "A payment processing system needs to handle concurrent transaction validation across multiple services. Design a concurrent validation pipeline where: 1) Transactions arrive via an input channel, 2) Three validation stages run concurrently (fraud check, balance check, compliance check) using the fan-out pattern, 3) All three validations must pass for a transaction to succeed (implement fan-in with result aggregation), 4) Each validation has a 2-second timeout using context.WithTimeout, 5) Use errgroup (golang.org/x/sync/errgroup) for coordinated error handling and cancellation. If any validation fails, cancel remaining checks. Provide working code with mock validators using time.Sleep, demonstrate proper error propagation, include at least two test cases (success and failure scenarios), and explain how you would identify performance bottlenecks using pprof.",
    
    "Your microservice has a shared connection pool that's causing race conditions and deadlocks. Implement a thread-safe connection pool with: 1) Maximum 10 connections using a semaphore pattern (buffered channel), 2) Acquire() and Release() methods protected by sync.Mutex for connection lifecycle management, 3) A background goroutine that health-checks idle connections every 5 seconds using time.Ticker, 4) Support for graceful drain during shutdown - reject new acquisitions, wait for active connections using sync.WaitGroup, then close all connections, 5) Implement proper cleanup using sync.Once for one-time initialization and defer for resource release. Provide complete code with 5 concurrent workers competing for connections, run with go test -race ",
    
    "You need to build a concurrent job scheduler that processes tasks with priority levels. Implement: 1) A priority queue (high, medium, low) using three separate channels, 2) A worker pool of 6 goroutines that processes jobs using select statement to prioritize high over medium over low, 3) Each job has a processing timeout (context.WithTimeout), and failed jobs should be retried once, 4) Use sync.RWMutex to maintain a thread-safe job status map (pending, running, completed, failed), 5) Implement metrics tracking (completed/failed counts) using sync/atomic operations. Provide working code that submits 30 jobs across different priorities, demonstrate proper shutdown with context cancellation ensuring in-flight jobs complete, and explain how you would use pprof and go tool trace to analyze scheduler behavior and identify goroutine contention.",
    
    "A real-time data synchronization service must replicate changes across multiple replica nodes concurrently. Design a system where: 1) A primary goroutine generates change events (inserts, updates, deletes), 2) Changes are broadcast to 4 replica goroutines using the fan-out pattern with buffered channels, 3) Replicas acknowledge processing by sending to an acknowledgment channel (fan-in), 4) The primary waits for at least 3 out of 4 acknowledgments before considering a change committed (quorum logic using select with timeout), 5) Use sync.Cond for efficient blocking when waiting for quorum and sync.Mutex to protect shared state. Implement graceful shutdown where primary stops sending new changes but waits for pending operations to complete using sync.WaitGroup. Provide complete code with proper producer-consumer pattern, demonstrate race-free operation with go test -race, handle the case where a replica becomes slow or unresponsive, and explain your choice between channels and mutexes for different synchronization points. Include comments on memory model implications and visibility guarantees."
  ],
  "Python-FastAPI(INTERMEDIATE),Redis(INTERMEDIATE)": [
    "Build a real-time leaderboard system for a quiz application. Using Redis sorted sets, implement endpoints to: submit a score (add/update user score), get top 10 players, get a specific user's rank and score, and get players ranked around a specific user (5 above and 5 below). Use Redis pipelining to optimize the 'get rank with neighbors' query. The solution should handle users with identical scores correctly and return proper responses when a user hasn't played yet.",
    
    "Implement a document editing lock system to prevent concurrent edits. Using Redis with WATCH/MULTI/EXEC, create endpoints to: acquire a lock on a document (store lock with user_id and timestamp in a hash, fail if already locked), release a lock (only if owned by requesting user), and force-release expired locks (older than 10 minutes). Use optimistic locking to handle race conditions when multiple users try to acquire the same lock simultaneously. Include lock status check endpoint.",
    
    "Create a multi-level cache invalidation system for a blog API. Use Redis hashes to cache individual posts and Redis sets to track tag-to-post relationships. Implement endpoints to: fetch a post (with caching), update a post (invalidate specific post cache and all related tag caches), and fetch posts by tag. Write a Lua script that atomically updates the cache and relationship sets to avoid partial updates during high concurrency. The cache should have different TTLs: posts (10 min), tag lists (5 min).",
    
    "Build a distributed circuit breaker for an external payment API. Using Redis hashes, track failure counts, last failure time, and circuit state (closed/open/half-open) per payment provider. Implement a decorator that: checks circuit state before calling payment API, increments failure count on errors, opens circuit after 5 failures in 1 minute, auto-transitions to half-open after 30 seconds, and closes circuit after successful half-open test. Use Lua script for atomic state transitions. Include endpoint to manually reset circuit.",
    
    "Implement a feature flag system with real-time updates across API instances. Store flags in Redis hashes with properties: enabled (bool), rollout_percentage (0-100), and allowed_user_ids (list). Create endpoints for: toggling flags, setting rollout percentage, and adding users to allowlist. Implement a flag evaluation function using Pydantic models that determines if a flag is enabled for a given user based on rules. Use Redis pub/sub to broadcast flag changes and update local in-memory cache. Include a dependency injection pattern for easy flag checking in routes."
  ],
  "ReactJs (INTERMEDIATE), ReactJs - Optimization (INTERMEDIATE)": [
    "A “compare products” screen preloads 1,000 product cards and their related resources (images, pricing, inventory) with fragmented React Query keys, causing cache misses and repeated refetch on focus/reconnect. Under scale it triggers duplicate queries from nested components and re-renders every card on any status update; optimize by consolidating query key design, gating with enabled, tuning staleTime/gcTime, and stabilizing props (memo/select) so network traffic and rerenders drop sharply."
    
  ]


}
